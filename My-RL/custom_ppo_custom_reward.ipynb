{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22d85411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All modules imported successfully!\n",
      "Using PyTorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import importlib\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import qwop_gym\n",
    "from pathlib import Path\n",
    "\n",
    "# Reload train_ppo module to pick up latest changes\n",
    "import train_ppo\n",
    "importlib.reload(train_ppo)\n",
    "\n",
    "# Import custom PPO components from train_ppo.py\n",
    "from train_ppo import (\n",
    "    ActorCritic,\n",
    "    RolloutBuffer,\n",
    "    create_qwop_env,\n",
    "    train_ppo,\n",
    "    evaluate_policy,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint\n",
    ")\n",
    "\n",
    "print(\"âœ“ All modules imported successfully!\")\n",
    "print(f\"Using PyTorch device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523ce749",
   "metadata": {},
   "source": [
    "# Custom PPO Training with Custom Reward Function\n",
    "\n",
    "Train a PPO agent with a custom reward wrapper that penalizes poor posture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86f9c9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ CustomRewardWrapper defined!\n",
      "\n",
      "Key features:\n",
      "  - Penalizes low torso height (bad posture)\n",
      "  - Adjustable penalty_scale parameter\n",
      "  - Set verbose=True to see reward breakdown\n",
      "\n",
      "Usage:\n",
      "  env = CustomRewardWrapper(env, penalty_scale=0.5, y_threshold=1.5, verbose=True)\n"
     ]
    }
   ],
   "source": [
    "# Custom Reward Wrapper\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Wrapper to customize the reward function for QWOP.\n",
    "    \n",
    "    Default reward is typically based on distance traveled.\n",
    "    You can modify this to add penalties, bonuses, or completely change the reward structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, penalty_scale=0.1, y_threshold=1.5, verbose=False):\n",
    "        super().__init__(env)\n",
    "        self.prev_distance = 0\n",
    "        self.steps_taken = 0\n",
    "        self.torso_y_idx = 1  # Index for torso Y-level in observation\n",
    "        self.y_threshold = y_threshold  # Minimum acceptable torso height\n",
    "        self.penalty_scale = penalty_scale  # Scale for posture penalty\n",
    "        self.verbose = verbose  # Print reward breakdown occasionally\n",
    "        self.total_base_reward = 0\n",
    "        self.total_penalty = 0\n",
    "        self.episode_steps = 0\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        if self.verbose and self.episode_steps > 0:\n",
    "            print(f\"\\n[CustomReward] Episode Summary:\")\n",
    "            print(f\"  Total base reward: {self.total_base_reward:.2f}\")\n",
    "            print(f\"  Total penalty: {self.total_penalty:.2f}\")\n",
    "            print(f\"  Penalty scale: {self.penalty_scale}\")\n",
    "            print(f\"  Average penalty per step: {self.total_penalty/self.episode_steps:.4f}\")\n",
    "        \n",
    "        self.prev_distance = 0\n",
    "        self.steps_taken = 0\n",
    "        self.total_base_reward = 0\n",
    "        self.total_penalty = 0\n",
    "        self.episode_steps = 0\n",
    "        return self.env.reset(**kwargs)\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, base_reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.steps_taken += 1\n",
    "        \n",
    "       # Get Torso Y-Level (Need to ensure this index is correct)\n",
    "        y_torso = obs[self.torso_y_idx]\n",
    "\n",
    "        posture_penalty = 0\n",
    "        if y_torso > self.y_threshold:\n",
    "            # Linear Penalty: -k * (y_threshold - y_torso)\n",
    "            posture_penalty = -self.penalty_scale * (self.y_threshold - y_torso)\n",
    "\n",
    "        custom_reward = base_reward + posture_penalty\n",
    "        \n",
    "        # Track cumulative rewards for debugging\n",
    "        self.total_base_reward += base_reward\n",
    "        self.total_penalty += posture_penalty\n",
    "        \n",
    "        # Verbose logging every 100 steps\n",
    "        if self.verbose and self.steps_taken % 100 == 0:\n",
    "            print(f\"Step {self.steps_taken}: y_torso={y_torso:.3f}, \"\n",
    "                  f\"base_reward={base_reward:.3f}, penalty={posture_penalty:.3f}, \"\n",
    "                  f\"total_reward={custom_reward:.3f}\")\n",
    "        \n",
    "        return obs, custom_reward, terminated, truncated, info\n",
    "\n",
    "print(\"âœ“ CustomRewardWrapper defined!\")\n",
    "print(\"\\nKey features:\")\n",
    "print(\"  - Penalizes low torso height (bad posture)\")\n",
    "print(\"  - Adjustable penalty_scale parameter\")\n",
    "print(\"  - Set verbose=True to see reward breakdown\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  env = CustomRewardWrapper(env, penalty_scale=0.5, y_threshold=1.5, verbose=True)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd18955",
   "metadata": {},
   "source": [
    "## Custom Reward Wrapper\n",
    "\n",
    "This wrapper modifies the reward function to penalize bad posture (low torso height).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0f95de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Environment created successfully!\n",
      "State dimension: 60\n",
      "Action dimension: 16\n",
      "Check your Brave browser - the QWOP game should be visible!\n"
     ]
    }
   ],
   "source": [
    "# Configuration for QWOP environment\n",
    "BROWSER_PATH = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "DRIVER_PATH = \"C:\\\\Program Files\\\\chromedriver-win64\\\\chromedriver-win64\\\\chromedriver.exe\"\n",
    "\n",
    "# Create QWOP environment using the helper function\n",
    "env = create_qwop_env(\n",
    "    browser_path=BROWSER_PATH,\n",
    "    driver_path=DRIVER_PATH,\n",
    "    stat_in_browser=True,\n",
    "    game_in_browser=True,\n",
    "    auto_draw=True,\n",
    "    frames_per_step=4,\n",
    "    max_episode_steps=2000,\n",
    "    text_in_browser=\"ðŸ¤– Training PPO Agent\"\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Environment created successfully!\")\n",
    "print(f\"State dimension: {env.observation_space.shape[0]}\")\n",
    "print(f\"Action dimension: {env.action_space.n}\")\n",
    "print(\"Check your Brave browser - the QWOP game should be visible!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed261f0",
   "metadata": {},
   "source": [
    "## Train PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cda0aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "\n",
    "# PPO Hyperparameters\n",
    "TOTAL_STEPS = 25000\n",
    "ROLLOUT_STEPS = 2048\n",
    "BATCH_SIZE = 256\n",
    "PPO_EPOCHS = 4\n",
    "GAMMA = 0.995\n",
    "GAE_LAMBDA = 0.95\n",
    "CLIP_COEF = 0.2\n",
    "LEARNING_RATE = 3e-4\n",
    "VALUE_COEF = 0.5\n",
    "ENTROPY_COEF = 0.01\n",
    "MAX_GRAD_NORM = 0.5\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "# Custom Reward Parameters\n",
    "PENALTY_SCALE = 0.5  # Increase this to make penalty more significant (try 0.5, 1.0, 2.0)\n",
    "Y_THRESHOLD = 1.5    # Torso height threshold (lower = more strict)\n",
    "REWARD_VERBOSE = True  # Set to True to see reward breakdown\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"../data/PPO-notebook\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc3b784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Custom reward wrapper applied!\n",
      "  Penalty scale: 0.5\n",
      "  Y threshold: 1.5\n",
      "  Verbose: True\n",
      "State dimension: 60, Action dimension: 16\n",
      "\n",
      "Starting PPO training...\n",
      "Total steps: 25,000\n",
      "Rollout steps: 2,048\n",
      "Watch the browser to see the agent learning!\n",
      "\n",
      "episode 1: reward=-10.12 len=  14\n",
      "episode 2: reward=-10.47 len=  18\n",
      "episode 3: reward=-11.56 len=  21\n",
      "episode 4: reward=-10.85 len=  33\n",
      "episode 5: reward=-10.46 len=  12\n",
      "episode 6: reward=-10.14 len=  11\n",
      "episode 7: reward=-10.43 len=  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-- 30199.66s [wsclient] WARNING: Failed to send/receive: timed out in 3.0s\n",
      "-- 30199.66s [wsclient] WARNING: Failed to send/receive: timed out in 3.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[info] KeyboardInterrupt received; saving final checkpoint...\n",
      "Training interrupted. Latest policy saved to ..\\data\\PPO-notebook\\ppo_final.pt\n",
      "\n",
      "============================================================\n",
      "TRAINING SUMMARY\n",
      "============================================================\n",
      "Total steps: 132\n",
      "Episodes completed: 7\n",
      "Average episode reward: -10.58\n",
      "Average episode length: 17.7\n",
      "Final model saved to: ..\\data\\PPO-notebook\\ppo_final.pt\n",
      "============================================================\n",
      "\n",
      "âœ“ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create fresh environment for training\n",
    "env = create_qwop_env(\n",
    "    browser_path=BROWSER_PATH,\n",
    "    driver_path=DRIVER_PATH,\n",
    "    stat_in_browser=True,\n",
    "    game_in_browser=True,\n",
    "    auto_draw=True,\n",
    "    frames_per_step=8,\n",
    "    max_episode_steps=500,\n",
    "    text_in_browser=\"ðŸ¤– Training PPO Agent\"\n",
    ")\n",
    "\n",
    "# Apply custom reward wrapper with specified parameters\n",
    "env = CustomRewardWrapper(env, penalty_scale=PENALTY_SCALE, y_threshold=Y_THRESHOLD, verbose=REWARD_VERBOSE)\n",
    "print(f\"âœ“ Custom reward wrapper applied!\")\n",
    "print(f\"  Penalty scale: {PENALTY_SCALE}\")\n",
    "print(f\"  Y threshold: {Y_THRESHOLD}\")\n",
    "print(f\"  Verbose: {REWARD_VERBOSE}\")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "print(f\"State dimension: {state_dim}, Action dimension: {action_dim}\")\n",
    "\n",
    "# Initialize policy and optimizer\n",
    "policy = ActorCritic(state_dim, action_dim, HIDDEN_SIZE).to(device)\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=LEARNING_RATE, eps=1e-5)\n",
    "\n",
    "print(\"\\nStarting PPO training...\")\n",
    "print(f\"Total steps: {TOTAL_STEPS:,}\")\n",
    "print(f\"Rollout steps: {ROLLOUT_STEPS:,}\")\n",
    "print(f\"Watch the browser to see the agent learning!\\n\")\n",
    "\n",
    "# Train the agent using the train_ppo function\n",
    "results = train_ppo(\n",
    "    env=env,\n",
    "    policy=policy,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    total_steps=TOTAL_STEPS,\n",
    "    rollout_steps=ROLLOUT_STEPS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    ppo_epochs=PPO_EPOCHS,\n",
    "    gamma=GAMMA,\n",
    "    gae_lambda=GAE_LAMBDA,\n",
    "    clip_coef=CLIP_COEF,\n",
    "    value_coef=VALUE_COEF,\n",
    "    entropy_coef=ENTROPY_COEF,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_every=10000,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Print training summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total steps: {results['total_steps']:,}\")\n",
    "print(f\"Episodes completed: {results['completed_episodes']}\")\n",
    "print(f\"Average episode reward: {results['average_reward']:.2f}\")\n",
    "print(f\"Average episode length: {results['average_length']:.1f}\")\n",
    "print(f\"Final model saved to: {results['final_checkpoint']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Close environment\n",
    "env.close()\n",
    "print(\"\\nâœ“ Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e091baa4",
   "metadata": {},
   "source": [
    "## Evaluate Trained Agent\n",
    "\n",
    "Load the trained model and watch it play QWOP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45f1cdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ../data/PPO-notebook/custom_ppo_final.pt\n",
      "âœ“ Model loaded (trained for 50000 steps)\n",
      "\n",
      "Evaluating agent for 5 episodes...\n",
      "Watch the browser to see your trained AI in action!\n",
      "\n",
      "Episode 1:\n",
      "âœ“ Model loaded (trained for 50000 steps)\n",
      "\n",
      "Evaluating agent for 5 episodes...\n",
      "Watch the browser to see your trained AI in action!\n",
      "\n",
      "Episode 1:\n",
      "  Steps: 2232\n",
      "  Distance: 30.40m\n",
      "  Total Reward: -61.66\n",
      "  âœ— Failed\n",
      "\n",
      "Episode 2:\n",
      "  Steps: 2232\n",
      "  Distance: 30.40m\n",
      "  Total Reward: -61.66\n",
      "  âœ— Failed\n",
      "\n",
      "Episode 2:\n",
      "  Steps: 2448\n",
      "  Distance: 32.12m\n",
      "  Total Reward: -67.71\n",
      "  âœ— Failed\n",
      "\n",
      "Episode 3:\n",
      "  Steps: 2448\n",
      "  Distance: 32.12m\n",
      "  Total Reward: -67.71\n",
      "  âœ— Failed\n",
      "\n",
      "Episode 3:\n",
      "  Steps: 1779\n",
      "  Distance: 26.37m\n",
      "  Total Reward: -49.71\n",
      "  âœ— Failed\n",
      "\n",
      "Episode 4:\n",
      "  Steps: 1779\n",
      "  Distance: 26.37m\n",
      "  Total Reward: -49.71\n",
      "  âœ— Failed\n",
      "\n",
      "Episode 4:\n",
      "  Steps: 1987\n",
      "  Distance: 24.10m\n",
      "  Total Reward: -58.36\n",
      "  âœ— Failed\n",
      "\n",
      "Episode 5:\n",
      "  Steps: 1987\n",
      "  Distance: 24.10m\n",
      "  Total Reward: -58.36\n",
      "  âœ— Failed\n",
      "\n",
      "Episode 5:\n",
      "  Steps: 1445\n",
      "  Distance: 18.33m\n",
      "  Total Reward: -44.55\n",
      "  âœ— Failed\n",
      "\n",
      "\n",
      "==================================================\n",
      "EVALUATION SUMMARY\n",
      "==================================================\n",
      "Average Distance: 26.27m\n",
      "Best Distance: 32.12m\n",
      "Average Reward: -56.40\n",
      "Success Rate: 0.0%\n",
      "==================================================\n",
      "  Steps: 1445\n",
      "  Distance: 18.33m\n",
      "  Total Reward: -44.55\n",
      "  âœ— Failed\n",
      "\n",
      "\n",
      "==================================================\n",
      "EVALUATION SUMMARY\n",
      "==================================================\n",
      "Average Distance: 26.27m\n",
      "Best Distance: 32.12m\n",
      "Average Reward: -56.40\n",
      "Success Rate: 0.0%\n",
      "==================================================\n",
      "\n",
      "âœ“ Evaluation complete!\n",
      "\n",
      "âœ“ Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Load and evaluate the trained PPO agent\n",
    "MODEL_PATH = \"../data/PPO-notebook/custom_ppo_final.pt\"  # Use custom trained model\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "print(f\"Loading model from: {MODEL_PATH}\")\n",
    "\n",
    "# Create evaluation environment\n",
    "eval_env = create_qwop_env(\n",
    "    browser_path=BROWSER_PATH,\n",
    "    driver_path=DRIVER_PATH,\n",
    "    stat_in_browser=True,\n",
    "    game_in_browser=True,\n",
    "    auto_draw=True,\n",
    "    frames_per_step=4,\n",
    "    max_episode_steps=5000,\n",
    "    text_in_browser=\"ðŸ¤– PPO Agent Evaluation\"\n",
    ")\n",
    "\n",
    "# Apply same custom reward wrapper (though rewards don't matter much during eval)\n",
    "eval_env = CustomRewardWrapper(eval_env, penalty_scale=PENALTY_SCALE, y_threshold=Y_THRESHOLD, verbose=False)\n",
    "\n",
    "# Initialize policy and load checkpoint\n",
    "state_dim = eval_env.observation_space.shape[0]\n",
    "action_dim = eval_env.action_space.n\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "eval_policy = ActorCritic(state_dim, action_dim, HIDDEN_SIZE).to(device)\n",
    "checkpoint = load_checkpoint(MODEL_PATH, eval_policy, device)\n",
    "\n",
    "print(f\"âœ“ Model loaded (trained for {checkpoint['step']} steps)\")\n",
    "print(\"\\nEvaluating agent for 5 episodes...\")\n",
    "print(\"Watch the browser to see your trained AI in action!\\n\")\n",
    "\n",
    "# Evaluate the policy\n",
    "results = evaluate_policy(\n",
    "    env=eval_env,\n",
    "    policy=eval_policy,\n",
    "    device=device,\n",
    "    num_episodes=5,\n",
    "    verbose=True,\n",
    "    render_delay=0.02\n",
    ")\n",
    "\n",
    "# Close environment\n",
    "eval_env.close()\n",
    "print(\"\\nâœ“ Evaluation complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
