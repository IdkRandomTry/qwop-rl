{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22d85411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All modules imported successfully!\n",
      "Using PyTorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries and custom PPO implementation\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gymnasium as gym\n",
    "import qwop_gym\n",
    "\n",
    "# Reload train_ppo module to pick up latest changes\n",
    "import importlib\n",
    "import train_ppo\n",
    "importlib.reload(train_ppo)\n",
    "\n",
    "# Import custom PPO components from train_ppo.py\n",
    "from train_ppo import (\n",
    "    ActorCritic,\n",
    "    RolloutBuffer,\n",
    "    create_qwop_env,\n",
    "    train_ppo,\n",
    "    evaluate_policy,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint\n",
    ")\n",
    "\n",
    "print(\"âœ“ All modules imported successfully!\")\n",
    "print(f\"Using PyTorch device: {torch.device('cuda' if torch.cuda.is_available() else 'cpu')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f9c9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ CustomRewardWrapper defined!\n",
      "\n",
      "Key features:\n",
      "  - Penalizes low torso height (bad posture)\n",
      "  - Adjustable penalty_scale parameter\n",
      "  - Set verbose=True to see reward breakdown\n",
      "\n",
      "Usage:\n",
      "  env = CustomRewardWrapper(env, penalty_scale=0.5, y_threshold=1.5, verbose=True)\n"
     ]
    }
   ],
   "source": [
    "# Custom Reward Wrapper\n",
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "    \"\"\"\n",
    "    Wrapper to customize the reward function for QWOP.\n",
    "    \n",
    "    Default reward is typically based on distance traveled.\n",
    "    You can modify this to add penalties, bonuses, or completely change the reward structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, penalty_scale=0.1, y_threshold=1.5, verbose=False):\n",
    "        super().__init__(env)\n",
    "        self.prev_distance = 0\n",
    "        self.steps_taken = 0\n",
    "        self.torso_y_idx = 1  # Index for torso Y-level in observation\n",
    "        self.y_threshold = y_threshold  # Minimum acceptable torso height\n",
    "        self.penalty_scale = penalty_scale  # Scale for posture penalty\n",
    "        self.verbose = verbose  # Print reward breakdown occasionally\n",
    "        self.total_base_reward = 0\n",
    "        self.total_penalty = 0\n",
    "        self.episode_steps = 0\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        if self.verbose and self.episode_steps > 0:\n",
    "            print(f\"\\n[CustomReward] Episode Summary:\")\n",
    "            print(f\"  Total base reward: {self.total_base_reward:.2f}\")\n",
    "            print(f\"  Total penalty: {self.total_penalty:.2f}\")\n",
    "            print(f\"  Penalty scale: {self.penalty_scale}\")\n",
    "            print(f\"  Average penalty per step: {self.total_penalty/self.episode_steps:.4f}\")\n",
    "        \n",
    "        self.prev_distance = 0\n",
    "        self.steps_taken = 0\n",
    "        self.total_base_reward = 0\n",
    "        self.total_penalty = 0\n",
    "        self.episode_steps = 0\n",
    "        return self.env.reset(**kwargs)\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, base_reward, terminated, truncated, info = self.env.step(action)\n",
    "        self.steps_taken += 1\n",
    "        \n",
    "       # Get Torso Y-Level (Need to ensure this index is correct)\n",
    "        y_torso = obs[self.torso_y_idx]\n",
    "\n",
    "        posture_penalty = 0\n",
    "        if y_torso > self.y_threshold:\n",
    "            # Linear Penalty: -k * (y_threshold - y_torso)\n",
    "            posture_penalty = -self.penalty_scale * (self.y_threshold - y_torso)\n",
    "\n",
    "        custom_reward = base_reward + posture_penalty\n",
    "        \n",
    "        # Track cumulative rewards for debugging\n",
    "        self.total_base_reward += base_reward\n",
    "        self.total_penalty += posture_penalty\n",
    "        \n",
    "        # Verbose logging every 100 steps\n",
    "        if self.verbose and self.steps_taken % 100 == 0:\n",
    "            print(f\"Step {self.steps_taken}: y_torso={y_torso:.3f}, \"\n",
    "                  f\"base_reward={base_reward:.3f}, penalty={posture_penalty:.3f}, \"\n",
    "                  f\"total_reward={custom_reward:.3f}\")\n",
    "        \n",
    "        return obs, custom_reward, terminated, truncated, info\n",
    "\n",
    "print(\"âœ“ CustomRewardWrapper defined!\")\n",
    "print(\"\\nKey features:\")\n",
    "print(\"  - Penalizes low torso height (bad posture)\")\n",
    "print(\"  - Adjustable penalty_scale parameter\")\n",
    "print(\"  - Set verbose=True to see reward breakdown\")\n",
    "print(\"\\nUsage:\")\n",
    "print(\"  env = CustomRewardWrapper(env, penalty_scale=0.5, y_threshold=1.5, verbose=True)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0f95de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Environment created successfully!\n",
      "State dimension: 60\n",
      "Action dimension: 16\n",
      "Check your Brave browser - the QWOP game should be visible!\n"
     ]
    }
   ],
   "source": [
    "# Configuration for QWOP environment\n",
    "BROWSER_PATH = \"C:\\\\Program Files\\\\BraveSoftware\\\\Brave-Browser\\\\Application\\\\brave.exe\"\n",
    "DRIVER_PATH = \"C:\\\\Program Files\\\\chromedriver-win64\\\\chromedriver-win64\\\\chromedriver.exe\"\n",
    "\n",
    "# Create QWOP environment using the helper function\n",
    "env = create_qwop_env(\n",
    "    browser_path=BROWSER_PATH,\n",
    "    driver_path=DRIVER_PATH,\n",
    "    stat_in_browser=True,\n",
    "    game_in_browser=True,\n",
    "    auto_draw=True,\n",
    "    frames_per_step=4,\n",
    "    max_episode_steps=2000,\n",
    "    text_in_browser=\"ðŸ¤– Training PPO Agent\"\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Environment created successfully!\")\n",
    "print(f\"State dimension: {env.observation_space.shape[0]}\")\n",
    "print(f\"Action dimension: {env.action_space.n}\")\n",
    "print(\"Check your Brave browser - the QWOP game should be visible!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca2cb271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing environment with 16 actions\n",
      "\n",
      "Initial observation shape: (60,)\n",
      "Initial info: {'time': np.float32(0.008433334), 'distance': np.float32(0.25110978), 'avgspeed': np.float32(29.775862), 'is_success': False}\n",
      "\n",
      "Step 0: Action=Q+W+O+P, Reward=0.06, Distance=0.26m\n",
      "Step 10: Action=Q+W+O+P, Reward=-0.03, Distance=0.39m\n",
      "\n",
      "Episode finished at step 13\n",
      "Final distance: 0.40m, Total reward: -10.21\n",
      "\n",
      "âœ“ Environment test complete!\n"
     ]
    }
   ],
   "source": [
    "# Action space mapping\n",
    "action_map = {\n",
    "    0: \"none\", 1: \"Q\", 2: \"W\", 3: \"O\", 4: \"P\",\n",
    "    5: \"Q+W\", 6: \"Q+O\", 7: \"Q+P\", 8: \"W+O\", 9: \"W+P\", 10: \"O+P\",\n",
    "    11: \"Q+W+O\", 12: \"Q+W+P\", 13: \"Q+O+P\", 14: \"W+O+P\", 15: \"Q+W+O+P\"\n",
    "}\n",
    "\n",
    "print(f\"Testing environment with {env.action_space.n} actions\\n\")\n",
    "\n",
    "obs, info = env.reset()\n",
    "print(f\"Initial observation shape: {obs.shape}\")\n",
    "print(f\"Initial info: {info}\\n\")\n",
    "\n",
    "# Take a few random actions\n",
    "total_reward = 0\n",
    "for step in range(50):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(f\"Step {step}: Action={action_map.get(action, 'UNKNOWN')}, \"\n",
    "              f\"Reward={reward:.2f}, Distance={info['distance']:.2f}m\")\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        print(f\"\\nEpisode finished at step {step + 1}\")\n",
    "        print(f\"Final distance: {info['distance']:.2f}m, Total reward: {total_reward:.2f}\")\n",
    "        break\n",
    "\n",
    "print(\"\\nâœ“ Environment test complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed261f0",
   "metadata": {},
   "source": [
    "## Train PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfc3b784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Custom reward wrapper applied!\n",
      "  Penalty scale: 0.5\n",
      "  Y threshold: 1.5\n",
      "  Verbose: True\n",
      "State dimension: 60, Action dimension: 16\n",
      "\n",
      "Starting PPO training...\n",
      "Total steps: 25,000\n",
      "Rollout steps: 2,048\n",
      "Watch the browser to see the agent learning!\n",
      "\n",
      "episode 1: reward=-10.12 len=  14\n",
      "episode 2: reward=-10.47 len=  18\n",
      "episode 3: reward=-11.56 len=  21\n",
      "episode 4: reward=-10.85 len=  33\n",
      "episode 5: reward=-10.46 len=  12\n",
      "episode 6: reward=-10.14 len=  11\n",
      "episode 7: reward=-10.43 len=  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-- 30199.66s [wsclient] WARNING: Failed to send/receive: timed out in 3.0s\n",
      "-- 30199.66s [wsclient] WARNING: Failed to send/receive: timed out in 3.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[info] KeyboardInterrupt received; saving final checkpoint...\n",
      "Training interrupted. Latest policy saved to ..\\data\\PPO-notebook\\ppo_final.pt\n",
      "\n",
      "============================================================\n",
      "TRAINING SUMMARY\n",
      "============================================================\n",
      "Total steps: 132\n",
      "Episodes completed: 7\n",
      "Average episode reward: -10.58\n",
      "Average episode length: 17.7\n",
      "Final model saved to: ..\\data\\PPO-notebook\\ppo_final.pt\n",
      "============================================================\n",
      "\n",
      "âœ“ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# PPO Hyperparameters\n",
    "TOTAL_STEPS = 25000\n",
    "ROLLOUT_STEPS = 2048\n",
    "BATCH_SIZE = 256\n",
    "PPO_EPOCHS = 4\n",
    "GAMMA = 0.995\n",
    "GAE_LAMBDA = 0.95\n",
    "CLIP_COEF = 0.2\n",
    "LEARNING_RATE = 3e-4\n",
    "VALUE_COEF = 0.5\n",
    "ENTROPY_COEF = 0.01\n",
    "MAX_GRAD_NORM = 0.5\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "# Custom Reward Parameters\n",
    "PENALTY_SCALE = 0.5  # Increase this to make penalty more significant (try 0.5, 1.0, 2.0)\n",
    "Y_THRESHOLD = 1.5    # Torso height threshold (lower = more strict)\n",
    "REWARD_VERBOSE = True  # Set to True to see reward breakdown\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"../data/PPO-notebook\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Setup device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create fresh environment for training\n",
    "env = create_qwop_env(\n",
    "    browser_path=BROWSER_PATH,\n",
    "    driver_path=DRIVER_PATH,\n",
    "    stat_in_browser=True,\n",
    "    game_in_browser=True,\n",
    "    auto_draw=True,\n",
    "    frames_per_step=8,\n",
    "    max_episode_steps=500,\n",
    "    text_in_browser=\"ðŸ¤– Training PPO Agent\"\n",
    ")\n",
    "\n",
    "# Apply custom reward wrapper with specified parameters\n",
    "env = CustomRewardWrapper(env, penalty_scale=PENALTY_SCALE, y_threshold=Y_THRESHOLD, verbose=REWARD_VERBOSE)\n",
    "print(f\"âœ“ Custom reward wrapper applied!\")\n",
    "print(f\"  Penalty scale: {PENALTY_SCALE}\")\n",
    "print(f\"  Y threshold: {Y_THRESHOLD}\")\n",
    "print(f\"  Verbose: {REWARD_VERBOSE}\")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "print(f\"State dimension: {state_dim}, Action dimension: {action_dim}\")\n",
    "\n",
    "# Initialize policy and optimizer\n",
    "policy = ActorCritic(state_dim, action_dim, HIDDEN_SIZE).to(device)\n",
    "optimizer = torch.optim.Adam(policy.parameters(), lr=LEARNING_RATE, eps=1e-5)\n",
    "\n",
    "print(\"\\nStarting PPO training...\")\n",
    "print(f\"Total steps: {TOTAL_STEPS:,}\")\n",
    "print(f\"Rollout steps: {ROLLOUT_STEPS:,}\")\n",
    "print(f\"Watch the browser to see the agent learning!\\n\")\n",
    "\n",
    "# Train the agent using the train_ppo function\n",
    "results = train_ppo(\n",
    "    env=env,\n",
    "    policy=policy,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    total_steps=TOTAL_STEPS,\n",
    "    rollout_steps=ROLLOUT_STEPS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    ppo_epochs=PPO_EPOCHS,\n",
    "    gamma=GAMMA,\n",
    "    gae_lambda=GAE_LAMBDA,\n",
    "    clip_coef=CLIP_COEF,\n",
    "    value_coef=VALUE_COEF,\n",
    "    entropy_coef=ENTROPY_COEF,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_every=10000,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Print training summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total steps: {results['total_steps']:,}\")\n",
    "print(f\"Episodes completed: {results['completed_episodes']}\")\n",
    "print(f\"Average episode reward: {results['average_reward']:.2f}\")\n",
    "print(f\"Average episode length: {results['average_length']:.1f}\")\n",
    "print(f\"Final model saved to: {results['final_checkpoint']}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Close environment\n",
    "env.close()\n",
    "print(\"\\nâœ“ Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e091baa4",
   "metadata": {},
   "source": [
    "## Evaluate Trained Agent\n",
    "\n",
    "Load the trained model and watch it play QWOP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f1cdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ../data/PPO-notebook/custom_ppo_final.pt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'create_qwop_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading model from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Create evaluation environment\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m eval_env = \u001b[43mcreate_qwop_env\u001b[49m(\n\u001b[32m      9\u001b[39m     browser_path=BROWSER_PATH,\n\u001b[32m     10\u001b[39m     driver_path=DRIVER_PATH,\n\u001b[32m     11\u001b[39m     stat_in_browser=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     12\u001b[39m     game_in_browser=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     13\u001b[39m     auto_draw=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     14\u001b[39m     frames_per_step=\u001b[32m4\u001b[39m,\n\u001b[32m     15\u001b[39m     max_episode_steps=\u001b[32m5000\u001b[39m,\n\u001b[32m     16\u001b[39m     text_in_browser=\u001b[33m\"\u001b[39m\u001b[33mðŸ¤– PPO Agent Evaluation\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Apply same custom reward wrapper (though rewards don't matter much during eval)\u001b[39;00m\n\u001b[32m     20\u001b[39m eval_env = CustomRewardWrapper(eval_env, penalty_scale=PENALTY_SCALE, y_threshold=Y_THRESHOLD, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'create_qwop_env' is not defined"
     ]
    }
   ],
   "source": [
    "# Load and evaluate the trained PPO agent\n",
    "MODEL_PATH = \"../data/PPO-notebook/custom_ppo_final.pt\"  # Use custom trained model\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "print(f\"Loading model from: {MODEL_PATH}\")\n",
    "\n",
    "# Create evaluation environment\n",
    "eval_env = create_qwop_env(\n",
    "    browser_path=BROWSER_PATH,\n",
    "    driver_path=DRIVER_PATH,\n",
    "    stat_in_browser=True,\n",
    "    game_in_browser=True,\n",
    "    auto_draw=True,\n",
    "    frames_per_step=4,\n",
    "    max_episode_steps=5000,\n",
    "    text_in_browser=\"ðŸ¤– PPO Agent Evaluation\"\n",
    ")\n",
    "\n",
    "# Apply same custom reward wrapper (though rewards don't matter much during eval)\n",
    "eval_env = CustomRewardWrapper(eval_env, penalty_scale=PENALTY_SCALE, y_threshold=Y_THRESHOLD, verbose=False)\n",
    "\n",
    "# Initialize policy and load checkpoint\n",
    "state_dim = eval_env.observation_space.shape[0]\n",
    "action_dim = eval_env.action_space.n\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "eval_policy = ActorCritic(state_dim, action_dim, HIDDEN_SIZE).to(device)\n",
    "checkpoint = load_checkpoint(MODEL_PATH, eval_policy, device)\n",
    "\n",
    "print(f\"âœ“ Model loaded (trained for {checkpoint['step']} steps)\")\n",
    "print(\"\\nEvaluating agent for 5 episodes...\")\n",
    "print(\"Watch the browser to see your trained AI in action!\\n\")\n",
    "\n",
    "# Evaluate the policy\n",
    "results = evaluate_policy(\n",
    "    env=eval_env,\n",
    "    policy=eval_policy,\n",
    "    device=device,\n",
    "    num_episodes=5,\n",
    "    verbose=True,\n",
    "    render_delay=0.02\n",
    ")\n",
    "\n",
    "# Close environment\n",
    "eval_env.close()\n",
    "print(\"\\nâœ“ Evaluation complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
